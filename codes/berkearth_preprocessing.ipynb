{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4c00a-020f-4a1c-938f-4a06492d90a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2003_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2004_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2005_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2006_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2007_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2008_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2009_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2010_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2011_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2012_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2013_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2014_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2015_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2016_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2017_v1.0.nc\n",
      "Processing file: BERKEARTH_maximum_temperature-anomaly_day_1x1_global_2018_v1.0.nc\n",
      "Combined data saved to: Data/csv/done/tmax.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the folder containing the NetCDF files\n",
    "folder_path = \"data_in_nc4/tmax_anomlay\"\n",
    "\n",
    "# List all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for file_name in file_list:\n",
    "    print(\"Processing file:\", file_name)  # Print the current file being processed\n",
    "    \n",
    "    # Open the NetCDF file\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    original_data = xr.open_dataset(file_path)\n",
    "\n",
    "    # Identify latitude and longitude coordinates or dimensions\n",
    "    lat_name = [dim for dim in original_data.coords if 'lat' in dim][0]\n",
    "    lon_name = [dim for dim in original_data.coords if 'lon' in dim][0]\n",
    "\n",
    "    # Define the target latitude and longitude grids with 0.25 spacing\n",
    "    target_lat = np.arange(6.5, 38.5, 0.25)  # Target latitude grid with 0.25 spacing\n",
    "    target_lon = np.arange(66.5, 100, 0.25)  # Target longitude grid with 0.25 spacing\n",
    "\n",
    "    # Regrid the data to the target grid without interpolating missing values\n",
    "    regridded_data = original_data.interp({lat_name: target_lat, lon_name: target_lon}, method='nearest')\n",
    "\n",
    "    # Select only the 'tasmax' variable\n",
    "    regridded_data = regridded_data['tasmax']\n",
    "\n",
    "    # Convert the regridded data to a pandas DataFrame\n",
    "    regridded_dataframe = regridded_data.to_dataframe().reset_index()  # Reset index to have time, lon, and lat as columns\n",
    "\n",
    "    # Rename columns\n",
    "    regridded_dataframe.rename(columns={'latitude': 'lat', 'longitude': 'lon', 'time': 'Date'}, inplace=True)\n",
    "\n",
    "    # Convert 'Date' column to datetime object\n",
    "    regridded_dataframe['Date'] = pd.to_datetime(regridded_dataframe['Date'])\n",
    "\n",
    "    # Format 'Date' column to YYYYMMDD\n",
    "    regridded_dataframe['Date'] = regridded_dataframe['Date'].dt.strftime('%Y%m%d')\n",
    "\n",
    "    # Rearrange columns\n",
    "    regridded_dataframe = regridded_dataframe[['lat', 'lon', 'Date', 'tasmax']] \n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(regridded_dataframe)\n",
    "\n",
    "# Concatenate all DataFrames in the list vertically\n",
    "combined_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "output_file_path = \"variables/tmax.csv\"\n",
    "combined_dataframe.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"Combined data saved to:\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e68c218d-f671-4a72-a80c-36b92d2026e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2003_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2004_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2005_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2006_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2007_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2008_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2009_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2010_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2011_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2012_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2013_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2014_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2015_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2016_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2017_v1.0.nc\n",
      "Processing file: BERKEARTH_minimum_temperature-anomaly_day_1x1_global_2018_v1.0.nc\n",
      "Combined data saved to: Data/csv/done/tmin.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the folder containing the NetCDF files\n",
    "folder_path = \"data_in_nc4/tmin\"\n",
    "\n",
    "# List all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for file_name in file_list:\n",
    "    print(\"Processing file:\", file_name)  # Print the current file being processed\n",
    "    \n",
    "    # Open the NetCDF file\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    original_data = xr.open_dataset(file_path)\n",
    "\n",
    "    # Identify latitude and longitude coordinates or dimensions\n",
    "    lat_name = [dim for dim in original_data.coords if 'lat' in dim][0]\n",
    "    lon_name = [dim for dim in original_data.coords if 'lon' in dim][0]\n",
    "\n",
    "    # Define the target latitude and longitude grids with 0.25 spacing\n",
    "    target_lat = np.arange(6.5, 38.5, 0.25)  # Target latitude grid with 0.25 spacing\n",
    "    target_lon = np.arange(66.5, 100, 0.25)  # Target longitude grid with 0.25 spacing\n",
    "\n",
    "    # Regrid the data to the target grid without interpolating missing values\n",
    "    regridded_data = original_data.interp({lat_name: target_lat, lon_name: target_lon}, method='nearest')\n",
    "\n",
    "    # Select only the 'tasmax' variable\n",
    "    regridded_data = regridded_data['tasmin']\n",
    "\n",
    "    # Convert the regridded data to a pandas DataFrame\n",
    "    regridded_dataframe = regridded_data.to_dataframe().reset_index()  # Reset index to have time, lon, and lat as columns\n",
    "\n",
    "    # Rename columns\n",
    "    regridded_dataframe.rename(columns={'latitude': 'lat', 'longitude': 'lon', 'time': 'Date'}, inplace=True)\n",
    "\n",
    "    # Convert 'Date' column to datetime object\n",
    "    regridded_dataframe['Date'] = pd.to_datetime(regridded_dataframe['Date'])\n",
    "\n",
    "    # Format 'Date' column to YYYYMMDD\n",
    "    regridded_dataframe['Date'] = regridded_dataframe['Date'].dt.strftime('%Y%m%d')\n",
    "\n",
    "    # Rearrange columns\n",
    "    regridded_dataframe = regridded_dataframe[['lat', 'lon', 'Date', 'tasmin']]\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(regridded_dataframe)\n",
    "\n",
    "# Concatenate all DataFrames in the list vertically\n",
    "combined_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "output_file_path = \"Data/csv/done/tmin.csv\"\n",
    "combined_dataframe.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"Combined data saved to:\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7959288b-c8dd-45af-83c6-9cdd6cf2e3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63a067-3c48-4836-9e58-cae36b418fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
