{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81968be4-44ec-4e73-82cf-a1b8989a5a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20030306__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20030306.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20180306__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20180306.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20150306__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20150306.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20160305__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20160305.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20170306__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20170306.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20140306__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20140306.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20130306__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20130306.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20120305__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20120305.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20110306__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20110306.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20100306__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20100306.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20090306__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20090306.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20080305__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20080305.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20070306__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20070306.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20060306__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20060306.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20050306__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20050306.csv\n",
      "Processed data_in_nc4/forest_c\\Forest_cover\\MODIS-TERRA_C6.1__MOD44B__ForestCoverFraction__LPDAAC__GLOBAL__0.5degree__UHAM-ICDC__20040305__fv0.01.nc and saved output to data_in_csv/new_csv/forest_cover_csv\\Forest_cover_unique_coordinates\\20040305.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the main folder containing all the year folders\n",
    "main_folder_path = \"data_in_nc4/forest_cover\"\n",
    "\n",
    "# Define the path to the output folder where all yearly output folders will be saved\n",
    "output_main_folder_path = \"raw_files/forest_cover_csv\"\n",
    "\n",
    "# Create the output folder if it does not exist\n",
    "os.makedirs(output_main_folder_path, exist_ok=True)\n",
    "\n",
    "# Iterate over year folders in the main folder\n",
    "for year_folder_name in os.listdir(main_folder_path):\n",
    "    # Construct the full path to the year folder\n",
    "    year_folder_path = os.path.join(main_folder_path, year_folder_name)\n",
    "    \n",
    "    # Check if the item in the main folder is indeed a directory\n",
    "    if os.path.isdir(year_folder_path):\n",
    "        # Define the path to the new folder where CSV files will be saved for this year\n",
    "        output_year_folder_path = os.path.join(output_main_folder_path, f\"{year_folder_name}_unique_coordinates\")\n",
    "        os.makedirs(output_year_folder_path, exist_ok=True)\n",
    "        \n",
    "        # Iterate over files in the year folder\n",
    "        for file_name in os.listdir(year_folder_path):\n",
    "            # Check if the file is a NetCDF file\n",
    "            if file_name.endswith(\".nc\"):\n",
    "                # Construct the full path to the file\n",
    "                file_path = os.path.join(year_folder_path, file_name)\n",
    "                \n",
    "                # Extract the date from the file name\n",
    "                file_date_parts = file_name.split('_')\n",
    "                file_date = None\n",
    "                for part in file_date_parts:\n",
    "                    if len(part) == 8 and part.isdigit():\n",
    "                        file_date = part\n",
    "                        break\n",
    "                \n",
    "                if file_date is None:\n",
    "                    print(f\"Unable to extract date from file name: {file_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Perform the operations as in the previous code snippet\n",
    "                original_data = xr.open_dataset(file_path)\n",
    "                lat_name = [dim for dim in original_data.coords if 'lat' in dim][0]\n",
    "                lon_name = [dim for dim in original_data.coords if 'lon' in dim][0]\n",
    "                target_lat = np.arange(6.5, 38.5, 0.25)\n",
    "                target_lon = np.arange(66.5, 100, 0.25)\n",
    "                regridded_data = original_data.interp({lat_name: target_lat, lon_name: target_lon}, method='nearest')\n",
    "                regridded_data = regridded_data[['forestcoverfraction']]\n",
    "                regridded_dataframe = regridded_data.to_dataframe().reset_index()\n",
    "                unique_coordinates = regridded_dataframe.drop_duplicates(subset=['lat', 'lon'])\n",
    "                csv_file_path = os.path.join(output_year_folder_path, f\"{file_date}.csv\")  # Corrected file name\n",
    "                unique_coordinates.to_csv(csv_file_path, index=False)\n",
    "                \n",
    "                print(f\"Processed {file_path} and saved output to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619c8bf-f99f-40bf-9ecb-630552ddcdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: 2003.csv\n",
      "Processed and saved: 2018.csv\n",
      "Processed and saved: 2015.csv\n",
      "Processed and saved: 2016.csv\n",
      "Processed and saved: 2017.csv\n",
      "Processed and saved: 2014.csv\n",
      "Processed and saved: 2013.csv\n",
      "Processed and saved: 2012.csv\n",
      "Processed and saved: 2011.csv\n",
      "Processed and saved: 2010.csv\n",
      "Processed and saved: 2009.csv\n",
      "Processed and saved: 2008.csv\n",
      "Processed and saved: 2007.csv\n",
      "Processed and saved: 2006.csv\n",
      "Processed and saved: 2005.csv\n",
      "Processed and saved: 2004.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def drop_first_column_and_save(directory):\n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file is a CSV file\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            \n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(filepath)\n",
    "            \n",
    "            # Drop the first column\n",
    "            df = df.iloc[:, 1:]\n",
    "            \n",
    "            # Save the modified DataFrame back to a CSV file with the same name\n",
    "            df.to_csv(filepath, index=False)\n",
    "            \n",
    "            print(f\"Processed and saved: {filename}\")\n",
    "\n",
    "# Specify the directory containing the CSV files\n",
    "directory = 'raw_files/forest_cover_csv/Forest_cover_unique_coordinates'\n",
    "\n",
    "# Call the function\n",
    "drop_first_column_and_save(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad9d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processed 2003.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2003_daily.csv\n",
      "INFO:root:Processed 2018.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2018_daily.csv\n",
      "INFO:root:Processed 2015.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2015_daily.csv\n",
      "INFO:root:Processed 2016.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2016_daily.csv\n",
      "INFO:root:Processed 2017.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2017_daily.csv\n",
      "INFO:root:Processed 2014.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2014_daily.csv\n",
      "INFO:root:Processed 2013.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2013_daily.csv\n",
      "INFO:root:Processed 2012.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2012_daily.csv\n",
      "INFO:root:Processed 2011.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2011_daily.csv\n",
      "INFO:root:Processed 2010.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2010_daily.csv\n",
      "INFO:root:Processed 2009.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2009_daily.csv\n",
      "INFO:root:Processed 2008.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2008_daily.csv\n",
      "INFO:root:Processed 2007.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2007_daily.csv\n",
      "INFO:root:Processed 2006.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2006_daily.csv\n",
      "INFO:root:Processed 2005.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2005_daily.csv\n",
      "INFO:root:Processed 2004.csv and saved to data_in_csv/new_csv/forest_cover_csv/hi\\2004_daily.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def process_files(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process CSV files containing forest cover data.\n",
    "\n",
    "    Args:\n",
    "    - input_folder (str): Path to the folder containing input CSV files.\n",
    "    - output_folder (str): Path to the folder where processed CSV files will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the output folder exists\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # List all CSV files in the input folder\n",
    "        files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "        for file in files:\n",
    "            # Extract the year from the filename\n",
    "            year = int(file.split('.')[0])  # Adjust this line if the filename format is different\n",
    "\n",
    "            # Read the data from the file\n",
    "            file_path = os.path.join(input_folder, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Create a DataFrame with all dates for the given year\n",
    "            start_date = datetime(year, 1, 1)\n",
    "            end_date = datetime(year, 12, 31)\n",
    "            date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "            # Ensure that the DataFrame contains 'Lat' and 'Lon' columns\n",
    "            if 'lat' not in df.columns or 'lon' not in df.columns:\n",
    "                raise ValueError(\"'lat' and 'lon' columns are missing in the DataFrame.\")\n",
    "\n",
    "            # Repeat each row in the DataFrame for each date in the date range\n",
    "            repeated_df = pd.concat([df] * len(date_range), ignore_index=True)\n",
    "\n",
    "            # Add the date column with the format YYYYMMDD\n",
    "            repeated_df['Date'] = date_range.strftime('%Y%m%d').tolist() * len(df)\n",
    "\n",
    "            # Reorder the columns\n",
    "            repeated_df = repeated_df[['lat', 'lon', 'Date', 'forestcoverfraction']]\n",
    "\n",
    "            # Save the new DataFrame to a file in the output folder\n",
    "            output_file_path = os.path.join(output_folder, f'{year}_daily.csv')\n",
    "            repeated_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "            logging.info(f'Processed {file} and saved to {output_file_path}')\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'An error occurred: {str(e)}')\n",
    "\n",
    "# Example usage:\n",
    "input_folder = 'raw_files/forest_cover_csv/Forest_cover_unique_coordinates'\n",
    "output_folder = 'raw_files/forest_cover_csv/daily_forest_cover'\n",
    "process_files(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c34f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "folder_path = 'raw_files/forest_cover_csv/daily_forest_cover'\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Rename columns\n",
    "        df.rename(columns={'lat': 'Lat', 'lon': 'Lon'}, inplace=True)\n",
    "        \n",
    "        # Append the data vertically\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_data.to_csv('variables/forestcover.csv', index=False)\n",
    "\n",
    "print(\"Combined CSV file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6256c409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
