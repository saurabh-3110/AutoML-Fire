{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5905096a",
   "metadata": {},
   "source": [
    "The variable names are follows:\n",
    "1. Wind_Speed_10m_Mean \n",
    "2. Relative_Humidity_2m_09h\n",
    "3. Cloud_Cover_Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284d517-87c2-4b10-a442-82a5b8701b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the main folder containing all the year folders\n",
    "main_folder_path = \"data_in_nc4/humidity\"  # Change according to the variable name\n",
    "\n",
    "# Define the path to the output folder where all yearly output folders will be saved\n",
    "output_main_folder_path = \"raw_files/humidity\"\n",
    "\n",
    "# Create the output folder if it does not exist\n",
    "os.makedirs(output_main_folder_path, exist_ok=True)\n",
    "\n",
    "# Iterate over year folders in the main folder\n",
    "for year_folder_name in os.listdir(main_folder_path):\n",
    "    # Construct the full path to the year folder\n",
    "    year_folder_path = os.path.join(main_folder_path, year_folder_name)\n",
    "    \n",
    "    # Check if the item in the main folder is indeed a directory\n",
    "    if os.path.isdir(year_folder_path):\n",
    "        # Define the path to the new folder where CSV files will be saved for this year\n",
    "        output_year_folder_path = os.path.join(output_main_folder_path, f\"{year_folder_name}_unique_coordinates\")\n",
    "        os.makedirs(output_year_folder_path, exist_ok=True)\n",
    "        \n",
    "        # Iterate over files in the year folder\n",
    "        for file_name in os.listdir(year_folder_path):\n",
    "            # Check if the file is a NetCDF file\n",
    "            if file_name.endswith(\".nc\"):\n",
    "                # Construct the full path to the file\n",
    "                file_path = os.path.join(year_folder_path, file_name)\n",
    "                \n",
    "                # Extract the date from the file name\n",
    "                file_date_parts = file_name.split('_')\n",
    "                file_date = None\n",
    "                for part in file_date_parts:\n",
    "                    if len(part) == 8 and part.isdigit():\n",
    "                        file_date = part\n",
    "                        break\n",
    "                \n",
    "                if file_date is None:\n",
    "                    print(f\"Unable to extract date from file name: {file_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Perform the operations as in the previous code snippet\n",
    "                original_data = xr.open_dataset(file_path)\n",
    "                lat_name = [dim for dim in original_data.coords if 'lat' in dim][0]\n",
    "                lon_name = [dim for dim in original_data.coords if 'lon' in dim][0]\n",
    "                target_lat = np.arange(6.5, 38.5, 0.25)\n",
    "                target_lon = np.arange(66.5, 100, 0.25)\n",
    "                regridded_data = original_data.interp({lat_name: target_lat, lon_name: target_lon}, method='nearest')\n",
    "                regridded_data = regridded_data[['Relative_Humidity_2m_09h']]  # Adjust according to the variable name\n",
    "                regridded_dataframe = regridded_data.to_dataframe().reset_index()\n",
    "                unique_coordinates = regridded_dataframe.drop_duplicates(subset=['lat', 'lon'])\n",
    "                csv_file_path = os.path.join(output_year_folder_path, f\"{file_date}.csv\")  # Corrected file name\n",
    "                unique_coordinates.to_csv(csv_file_path, index=False)\n",
    "                \n",
    "                print(f\"Processed {file_path} and saved output to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a47956-7684-4e4e-86c1-f3f414b3cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the yearly output folder\n",
    "yearly_output_folder = \"raw_files/humidity\"\n",
    "\n",
    "# Iterate over year folders in the yearly output folder\n",
    "for year_folder_name in os.listdir(yearly_output_folder):\n",
    "    # Construct the full path to the year folder\n",
    "    year_folder_path = os.path.join(yearly_output_folder, year_folder_name)\n",
    "    \n",
    "    # Check if the item in the yearly output folder is indeed a directory\n",
    "    if os.path.isdir(year_folder_path):\n",
    "        # Collect all CSV files in the year folder\n",
    "        csv_files = [file for file in os.listdir(year_folder_path) if file.endswith('.csv')]\n",
    "        \n",
    "        # Initialize an empty DataFrame to hold the combined data\n",
    "        combined_data = pd.DataFrame(columns=['lat', 'lon'])\n",
    "        \n",
    "        # Iterate over each CSV file in the year folder\n",
    "        for csv_file_name in csv_files:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            csv_file_path = os.path.join(year_folder_path, csv_file_name)\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            \n",
    "            # Extract the date from the CSV file name\n",
    "            date = csv_file_name.split('.')[0]\n",
    "            \n",
    "            # Rename the 'SoilMoist_RZ_tavg' column to the date\n",
    "            df.rename(columns={'Relative_Humidity_2m_09h': date}, inplace=True) # Adjust according to the variable name\n",
    "            \n",
    "            # Merge the current DataFrame with the combined DataFrame based on lat and lon\n",
    "            combined_data = pd.merge(combined_data, df[['lat', 'lon', date]], on=['lat', 'lon'], how='outer')\n",
    "        \n",
    "        # Reorder the columns to have lat and lon as the first two columns\n",
    "        combined_data = combined_data[['lat', 'lon'] + [col for col in combined_data.columns if col not in ['lat', 'lon']]]\n",
    "        \n",
    "        # Save the combined DataFrame to a CSV file\n",
    "        output_csv_path = os.path.join(yearly_output_folder, f\"{year_folder_name}.csv\")\n",
    "        combined_data.to_csv(output_csv_path, index=False)\n",
    "        \n",
    "        print(f\"Combined data for {year_folder_name} and saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d773cecc",
   "metadata": {},
   "source": [
    "Save the yearly files seaprately to a folder (yearly_files/humidity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6538f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def transpose_csv(input_folder, output_folder):\n",
    "    # Walk through the directory tree\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for filename in files:\n",
    "            # Skip hidden macOS metadata files\n",
    "            if filename.endswith('.csv') and not filename.startswith('._'):\n",
    "                input_file = os.path.join(root, filename)\n",
    "                try:\n",
    "                    # Read the CSV file into a pandas DataFrame\n",
    "                    df = pd.read_csv(input_file)\n",
    "                    \n",
    "                    # Transpose the DataFrame\n",
    "                    df_transposed = df.T\n",
    "                    \n",
    "                    # Construct the output folder path mirroring the input folder structure\n",
    "                    relative_path = os.path.relpath(root, input_folder)\n",
    "                    output_subfolder = os.path.join(output_folder, relative_path)\n",
    "                    \n",
    "                    # Create the output subfolder if it doesn't exist\n",
    "                    os.makedirs(output_subfolder, exist_ok=True)\n",
    "                    \n",
    "                    # Generate the output filename\n",
    "                    output_file = os.path.join(output_subfolder, filename)\n",
    "                    \n",
    "                    # Write the transposed DataFrame to a new CSV file\n",
    "                    df_transposed.to_csv(output_file, index=True, header=False)\n",
    "                    print(f\"Transposed and saved: {input_file} -> {output_file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {input_file}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"yearly_files/humidity\"\n",
    "    output_folder = \"transposed/humidity\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    transpose_csv(input_folder, output_folder)\n",
    "    print(\"Transposition complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617200cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def transform_files(input_folder, output_file):\n",
    "    # Initialize an empty DataFrame to store transformed data from all files\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate through each file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        # Skip hidden metadata files\n",
    "        if filename.endswith('.csv') and not filename.startswith('._'):\n",
    "            input_file = os.path.join(input_folder, filename)\n",
    "            try:\n",
    "                # Read the file and transform the data\n",
    "                transformed_df = transform_file(input_file)\n",
    "                # Append transformed data to the combined DataFrame\n",
    "                combined_df = pd.concat([combined_df, transformed_df], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {input_file}: {e}\")\n",
    "\n",
    "    # Write the combined DataFrame to a single output file\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(\"Transformation complete. Combined data saved to:\", output_file)\n",
    "\n",
    "def transform_file(input_file):\n",
    "    transformed_rows = []\n",
    "\n",
    "    with open(input_file, 'r') as f_in:\n",
    "        reader = csv.reader(f_in)\n",
    "        latitudes = next(reader)\n",
    "        longitudes = next(reader)\n",
    "        # Skip the header row\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            date = row[0]\n",
    "            values = row[1:]\n",
    "            for lat, lon, value in zip(latitudes, longitudes, values):\n",
    "                transformed_rows.append([lat, lon, date, value])\n",
    "\n",
    "    # Convert transformed rows to a DataFrame\n",
    "    transformed_df = pd.DataFrame(transformed_rows, columns=['Lat', 'Lon', 'Date', 'humidity'])  # Adjust column names as needed\n",
    "    return transformed_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = 'transposed/humidty'  # Specify the input folder containing CSV files\n",
    "    output_file = 'variables/cloudcover.csv'  # Specify the output file name\n",
    "    \n",
    "    transform_files(input_folder, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
